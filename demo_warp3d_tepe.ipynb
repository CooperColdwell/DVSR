{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7420c5ce-3615-44bc-8f2e-e11d5bcc1654",
   "metadata": {},
   "source": [
    "## Pre-requisite: install pytorch3d package:\n",
    "\n",
    "Please follow the installation guide: https://github.com/facebookresearch/pytorch3d/blob/main/INSTALL.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffbfbd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pytorch3d.renderer import (\n",
    "    AlphaCompositor,\n",
    "    PerspectiveCameras,\n",
    "    PointsRasterizationSettings,\n",
    "    PointsRasterizer,\n",
    "    PointsRenderer,\n",
    ")\n",
    "from pytorch3d.structures import Pointclouds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a125ff1d",
   "metadata": {},
   "source": [
    "## Define 3D renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2415c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointsRendererWithDepth(PointsRenderer):\n",
    "    \"\"\"Augment PointsRenderer to output depth\"\"\"\n",
    "\n",
    "    def __init__(self, rasterizer, compositor) -> None:\n",
    "        super(PointsRendererWithDepth, self).__init__(rasterizer, compositor)\n",
    "\n",
    "    def forward(self, point_clouds, **kwargs) -> torch.Tensor:\n",
    "        fragments = self.rasterizer(point_clouds, **kwargs)\n",
    "\n",
    "        # Construct weights based on the distance of a point to the true point.\n",
    "        # However, this could be done differently: e.g. predicted as opposed\n",
    "        # to a function of the weights.\n",
    "        r = self.rasterizer.raster_settings.radius\n",
    "\n",
    "        dists2 = fragments.dists.permute(0, 3, 1, 2)\n",
    "        weights = 1 - dists2 / (r * r)\n",
    "        images = self.compositor(\n",
    "            fragments.idx.long().permute(0, 3, 1, 2),\n",
    "            weights,\n",
    "            point_clouds.features_packed().permute(1, 0),\n",
    "            **kwargs,\n",
    "        )\n",
    "        zbuf = fragments.zbuf.permute(0, 3, 1, 2)\n",
    "        \n",
    "        return images, F.relu(zbuf)\n",
    "\n",
    "\n",
    "renderer = PointsRendererWithDepth(\n",
    "            rasterizer=PointsRasterizer(),\n",
    "            compositor=AlphaCompositor(background_color=(0, 0, 0)),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af88536e",
   "metadata": {},
   "source": [
    "## Depth Warping in 3D\n",
    "\n",
    "**NOTE**: We warp depth from previous frame to current frame, and assume the depth value do not change (warped depth). The warping only requires 2D optical flow information, but it needs to be done in 3D to utilize the z-buffer.\n",
    "\n",
    "Then the difference between the current frame depth and the warped depth is calculated. We compare this depth difference in ground truth sequence and predicted sequence to get the TEPE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5565ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_depth(depth, depth_prev, flow):\n",
    "    \n",
    "    # create u, v coordinate in previous frame\n",
    "    h, w = depth.shape[:2]\n",
    "    u_prev, v_prev = np.meshgrid(np.arange(w), np.arange(h))\n",
    "    \n",
    "    # warp to current frame with flow\n",
    "    u = u_prev + flow[...,0]\n",
    "    v = v_prev + flow[...,1]\n",
    "    \n",
    "    \n",
    "    # camera intrinsics can be set to arbitrary\n",
    "    fx = w\n",
    "    fy = w\n",
    "    cx = w/2\n",
    "    cy = h/2\n",
    "    intrinsics = torch.Tensor([fx,fy,cx,cy]).unsqueeze(0).float().to(device)\n",
    "    \n",
    "    # create point cloud assuming depth does not change from previous frame to current frame\n",
    "    x = depth_prev * (u - cx)/fx\n",
    "    y = depth_prev * (v - cy)/fy\n",
    "    z = depth_prev\n",
    "    \n",
    "    pc = np.concatenate([x[...,np.newaxis], y[...,np.newaxis], z[...,np.newaxis]], axis = 2)\n",
    "    \n",
    "    # project\n",
    "    pc = torch.from_numpy(pc).unsqueeze(0).float().to(device)\n",
    "    \n",
    "    # create pc\n",
    "    B = intrinsics.shape[0]\n",
    "    verts = pc.reshape(B, -1, 3).contiguous()\n",
    "    feat = torch.ones(B, h, w, 1).reshape(B, -1, 1).to(device) # dummy feature to warp\n",
    "    verts[..., 0] = verts[..., 0] * -1\n",
    "    verts[..., 1] = verts[..., 1] * -1\n",
    "    point_cloud = Pointclouds(points=verts, features=feat)\n",
    "    \n",
    "    cameras = PerspectiveCameras(\n",
    "        device=device,\n",
    "        principal_point=intrinsics[:, -2:],\n",
    "        focal_length=intrinsics[:, :2],\n",
    "        image_size=((h, w),),\n",
    "        in_ndc=False,\n",
    "    )\n",
    "\n",
    "    radius = 2 # set rendering radius = 2 to avoid holes\n",
    "    raster_settings = PointsRasterizationSettings(\n",
    "        image_size=(h, w),\n",
    "        radius=1.0\n",
    "        / h\n",
    "        * radius,  # The radius (in NDC units) of the disk to be rasterized.\n",
    "        points_per_pixel=1,\n",
    "    )\n",
    "    renderer.rasterizer.cameras = cameras\n",
    "    renderer.rasterizer.raster_settings = raster_settings\n",
    "    feat_warp, zbuf = renderer(\n",
    "        point_cloud,\n",
    "        gamma=(1e-4,),\n",
    "        background_color=torch.tensor(\n",
    "            [0.0], dtype=torch.float32, device=device\n",
    "        ),\n",
    "        eps=1e-5,\n",
    "    )\n",
    "    \n",
    "    # valid mask is calculated from the dummy feature warping\n",
    "    valid_mask = (feat_warp > 0).float()\n",
    "    \n",
    "    # delta depth is calculated from the current frame depth and the warped depth\n",
    "    depth = torch.from_numpy(depth).unsqueeze(0).unsqueeze(1).float().to(device)\n",
    "    delta_depth = depth - zbuf\n",
    "    \n",
    "    return valid_mask, delta_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad3e151",
   "metadata": {},
   "source": [
    "## Load GT depth sequence and calcualte delta depth sequence\n",
    "\n",
    "Pre-requisite: please download the example sequence: https://drive.google.com/drive/folders/14cbm6HUrbuBpRgdMPkPNU53ISF9xakMI?usp=drive_link \n",
    "\n",
    "Unzip, and copy it into the `data` directory\n",
    "\n",
    "Since the TEPE metric calculation requires ground truth optical flow, we only calcualte it on **TarTanAir dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9897579",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "seq_len = 29 # calcualte metric for first 30 frames\n",
    "\n",
    "valid_masks = []\n",
    "delta_depth_gts = []\n",
    "\n",
    "for fidx in range(seq_len): \n",
    "    # depth at previous frame\n",
    "    depth_gt_prev = np.load('data/tartan_office_P003/depth/{:06d}.npy'.format(fidx))\n",
    "    # depth at current frame\n",
    "    depth_gt = np.load('data/tartan_office_P003/depth/{:06d}.npy'.format(fidx+1))\n",
    "    # 2D optical flow from previous frame to current frame\n",
    "    flow = np.load('data/tartan_office_P003/flow/{:06d}_{:06d}_flow.npy'.format(fidx, fidx+1))\n",
    "    \n",
    "    valid_mask, delta_depth_gt = get_delta_depth(depth_gt, depth_gt_prev, flow)\n",
    "    \n",
    "    valid_masks.append(valid_mask[:,0])\n",
    "    delta_depth_gts.append(delta_depth_gt[:,0])\n",
    "\n",
    "valid_masks = torch.cat(valid_masks, dim = 0)\n",
    "delta_depth_gts = torch.cat(delta_depth_gts, dim = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d435069b",
   "metadata": {},
   "source": [
    "## Load predicted depth sequence and calcualte delta depth sequence\n",
    "\n",
    "The prediction sequence can be generated with the command (after downloading the ground truth sequence):\n",
    "\n",
    "`python video_demo.py configs/dvsr_config.py chkpts/dvsr_tartan.pth data/tartan_office_P003 results/tartan_office_P003 --device 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a401c1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_depth_preds = []\n",
    "\n",
    "for fidx in range(seq_len):\n",
    "    # predicted depth at previous frame\n",
    "    depth_pred_prev = np.load('results/tartan_office_P003/{:08d}.npy'.format(fidx))[0,0]*10.0\n",
    "    # predicted depth at current frame\n",
    "    depth_pred = np.load('results/tartan_office_P003/{:08d}.npy'.format(fidx+1))[0,0]*10.0\n",
    "    # 2D optical flow from previous frame to current frame\n",
    "    flow = np.load('data/tartan_office_P003/flow/{:06d}_{:06d}_flow.npy'.format(fidx, fidx+1))\n",
    "    \n",
    "    _, delta_depth_pred = get_delta_depth(depth_pred, depth_pred_prev, flow)\n",
    "    \n",
    "    delta_depth_preds.append(delta_depth_pred[:,0])\n",
    "\n",
    "delta_depth_preds = torch.cat(delta_depth_preds, dim = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721a40c1",
   "metadata": {},
   "source": [
    "## Calculate TEPE metric (L1 between delta depth sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b949a01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEPE metric:  tensor(0.0077, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tepe = torch.mean(torch.abs((delta_depth_gts - delta_depth_preds) * valid_masks))\n",
    "print('TEPE metric: ', tepe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "pytorch3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
